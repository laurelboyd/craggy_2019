---
title: "Mapping rates of homelessness in the United States"
author: "[Laurel Boyd](https://github.com/laurelboyd)"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# How does King County compare?

This analysis attempts to answer the question of how seattle/King County is doing relative to the rest of the country in addressing homelessness, using only publically available data.

# Point In Time Counts

Point in time (PIT) surveys are required by the United States Department of Housing and Urban Development (HUD) for receipt of monies to provide homeless services. Although comparison between regions is difficult due to the different methodologies accepted by HUD, without other data sources, it is a reasonably proxy for assessing relative differences in homelessness throughout the country. [^1] 

In order to compare estimated rates of homelessness across the US, we need to account for small populations (which inflate rates if unadjusted). For this analysis, we'll use the most recent Census population estimates from the American Community Survey and will adjust for small populations using Global Empirical Bayes (EB) rate estimate.

[^1] https://files.hudexchange.info/resources/documents/PIT-Count-Methodology-Guide.pdf

This is a multi-step process:

Load data:

* 2017 Point in time counts for the entire nation by CoC region
* CoC shapefiles from HUD
* County population estimates from most recent Census (American Community Survey estimates from 2017)

Do some spatial math:

* Smooth population counts across census tracts.
* Overlay CoC regions on top of census tracts and calculate population for each region. (Unininsured would be interesting to look at if there's time.)
* Divide homeless counts by total population to calculate Global Empirical Bayes (EB) rate estimate.
* Map findings.



```{r load packages}
library(dplyr) #data manipulation
library(rio) #speedy data import tool
library(leaflet) #mapping package
library(mapview) #mapping package (leaflet wrapper)
library(httr) #need this to use "GET" call for getting CoC geodatabase
library(rgdal) # mapping statistics/functions
library(sf) # "spatial features" do some mapping (this is the newer package)
library(sp) # do some mapping (this is the older package)
library(ggplot2) #another way to make maps 
library(janitor) #cleaning, tables with totals
library(stringr) #regex functionality in dplyr
library(tidycensus) #Census population estimates
```
```{r }
###################################################################################
####################  Import PIT counts ###########################################

#(Craggy note: This portion would ideally come from the PIT code in the repository - but it didn't work for me so I just grabbed the portion I need for mapping)

fieldmap <- rio::import(system.file("extdata", "pit_counts_coc_fieldmap_2007t2018.xlsx", package = "craggy2019"))

file <- system.file("extdata", "2007-2018-PIT-Counts-by-CoC.xlsx", package = "craggy2019")
y2017 <- rio::import(file, which = 2)[,fieldmap$y2017] #207 since that's the most recent year of Census estimates...
names(y2017) <- fieldmap$shortname
y2017$year <- 2017

#400 items; 1 NA column and 1 gobbly gook: "a MO-604 covers territory in both Missouri..."
#y2017 %>% distinct(coc_number)

#drop down to 398
y2017 <-y2017 %>%
  filter(!str_detect(coc_number, 'a MO') & (!is.na(coc_number)))

#need numeric columns for joining to CoC regions
y2017$overall_homeless <- as.numeric(as.character(y2017$overall_homeless))

```

```{r }
###################################################################################
####################  Import CoC regions and join PIT counts#######################

# HUD input file geodatabase
url <- "https://www.hudexchange.info/resource/coc/gis/CoC_GIS_NatlTerrDC_Shapefile_2017.zip"
GET(url, write_disk("data.zip", overwrite = TRUE))
unzip("data.zip", overwrite = TRUE) #unzip file from downloads

# List all feature classes in a file geodatabase (only one feature class in this geodatabase)
#ogrListLayers("FY17_CoC_National_Bnd.gdb")

#Check out what format the data is in (should be SpatialPolygonsDataFrame)
#class(FY17COC)

# Read in the feature class
x <- st_read(dsn="FY18_CoC_National_Bnd.gdb")

# Rename column names in geodatabase layer to match PIT counts file
names(x)[names(x) == 'COCNUM'] <- 'coc_number'

#join 2017 PIT counts to map before converting to a spatal layer
y <- dplyr::inner_join(x, y2017)
z <- st_zm(y)
FY17COC <- as(z, "Spatial") #SpatialPolygonsDataFrame # "sp" type
#sf::st_crs(FY17COC) #check coordinate reference system (CRS)
FY17COC <- sf::st_as_sf(FY17COC) #convert to "sf" type
#class(FY17COC) #yep, now "sf"

#map CoC regions
#mapview(FY17COC)

#is this needed?
#FY17COC <- st_transform(FY17COC, CRS("+proj=longlat +datum=WGS84")) #need to define datum for use by leaflet
#sf::st_crs(FY17COC) #check coordinate reference system
   
```

```{r }
###################################################################################
####################  Import census data ##########################################

#load population counts fron tidycensus package (most recent American Community Survey data is from 2017)
#http://zevross.com/blog/2018/10/02/creating-beautiful-demographic-maps-in-r-with-the-tidycensus-and-tmap-packages/

#step 1: request Census API: https://api.census.gov/data/key_signup.html
#step 2: figure out which table (total population counts) to import (e.g. "B######")-> go to  https://censusreporter.org/
#step 3: join CoC regions and counties and aggregate population counts at CoC region level
#step 4: run spatial statistics and map findings

#STEP 1:
#census_api_key("YOUR KEY GOES HERE", install = TRUE)
#readRenviron("~/.Renviron")#run this the first time you use this key
#Sys.getenv("CENSUS_API_KEY")#Check yer work!

#STEPS 2-3:
POPCOUNTS <- get_acs("county", table= "B00001", year = 2017,
 output = "tidy", state = NULL, geometry = TRUE, shift_geo = FALSE) %>% #add cache_table = TRUE first time you run this
  rename (`popcount` = estimate) 
POPCOUNTS <- st_transform(POPCOUNTS, CRS("+proj=longlat +datum=WGS84")) #need to define datum for use by leaflet
POPCOC <- st_join(FY17COC,POPCOUNTS)

mapview(POPCOC)

#mapview(POPCOUNTS) + mapview(FY17COC)


```
```{r }
###################################################################################
####################  Calculate rates and do some spatial statistics ##############

#helpful for thinking about which mapping strategy makes sense:
# if using rural areas, adjust for low population counts
# https://mgimond.github.io/Spatial/mapping-rates-in-r.html



#what's the breakdown of homelessnes throughout the CoC's?
#summary(FY18COC$overall_homeless)
#tabyl(FY18COC$overall_homeless)


# https://rstudio.github.io/leaflet/choropleths.html

#bins <- c(0, 500, 1000, 5000, 10000, 20000, 40000, Inf)
#pal <- colorBin("PuRd", domain = FY18COC$overall_homeless, bins = bins)

# labels <- sprintf(
#   "<strong>%s</strong><br/>%g people / mi<sup>2</sup>",
#   states$name, states$density
# ) %>% lapply(htmltools::HTML)

# m <- m %>% addPolygons(
#   fillColor = ~pal(overall_homeless),
#   weight = 2,
#   opacity = 1,
#   color = "white",
#   dashArray = "3",
#   fillOpacity = 0.7,
#   highlight = highlightOptions(
#     weight = 5,
#     color = "#666",
#     dashArray = "",
#     fillOpacity = 0.7,
#     bringToFront = TRUE))%>%
#    addLegend(pal = pal, values = ~overall_homeless, opacity = 0.7, title = NULL,
#   position = "bottomright")
# m

```

## How does Seattle CoC compare with other areas of the US?



